# 大数据概述

### 内容

- 三次信息化浪潮：信息处理、信息传输、信息爆炸
- 大数据意指一个超大的、难以用现有常规的数据库管理技术和工具处理的数据集。
  - <img src="https://thdlrt.oss-cn-beijing.aliyuncs.com/image-20230918141311606.png" alt="image-20230918141311606" style="zoom:33%;" />

- 大数据分类：
  - 结构：结构化、半结构化、非结构化
  - 获取和处理方式：动态/实时数据；静态非实时数据
  - 关联特征：无关联/简单关联（键值数据）；复杂关联（图数据）

- 软件是大数据的驱动力

- 大数据涉及的技术：
  - 海量数据存储技术：分布式文件系统
  - 实时数据处理技术：流计算引擎
  - 数据高速传输技术：服务器/存储间高速通信
  - 搜索技术：文本检索、智能搜索、实时搜
    索
  - 数据分析技术：自然语言处理、文本情感分析、机器学习、聚类关联、数据模型

### 题目

#### 1.请简要介绍四种科学研究范式。

- 实验科学：强调通过**实验**来收集数据和验证假设的科学方法。。
- 理论科学：理论科学侧重于**构建理论模型和框架**，以解释自然现象或社会现象的原理和机制，而不一定依赖于实验数据。
- 计算科学：**使用计算机来模拟和分析**复杂的自然或社会现象，建立数学模型，通过计算机编程求解问题。
- 数据科学：数据科学涵盖了数据收集、清理、分析和解释等过程，以**从大规模数据集中提取知识和见解**，使用机器学习数据挖掘等方式获取结论和信息。

#### 2.请解释数据、信息、知识的区别和关系。

- 数据：数据是**原始、未加工**的**事实**、数字、符号或描述性信息的集合。
- 信息：信息是**对数据进行组织、解释和赋予意义后的产物**。信息通常有上下文，具有**可理解**的含义，并可用于做出决策或执行某些行动。
- 知识：知识是**更高级别的信息**，是通过将信息**与其他信息或经验相结合来建立的**。它涉及对信息的理解、分析、评估和应用，以生成新的见解、规则或模式。
- 总的来说，数据是信息的基础，而信息又是知识的基础。知识是更高层次的认知构建，它不仅包含了信息的含义，还具备了应用和理解的能力。

#### 3.请简述结构化数据/半结构化数据/非结构化数据的区别。

- **结构化数据**：结构化数据是按照明确定义的数据模型或架构进行组织和存储的数据。这些数据以表格、数据库或**电子表格**等形式存在，通常包括行和列的结构，每列都有特定的数据类型和字段。
- **半结构化数据**：半结构化数据是介于结构化数据和非结构化数据之间的数据类型。它们没有固定的表格结构，但通常包含标记、标签或其他结构性元素（如**JSON、XML**等），以便在数据中识别和提取信息。
- **非结构化数据**：非结构化数据是没有明确结构或组织的数据，通常以自由**文本、图像、音频、视频或自然语言的形式存在**。这种数据类型不容易按行和列或其他规则进行组织。

#### 4.大数据有哪几个特征？（5v）

- **大体量** Volume：数据规模很大
- **多样性** Varity：包含不同格式和形态的数据
- **时效性** Velocity：需要在一定的可接受的时间内完成数据的处理
- **准确性** Veracity：结果要保证一定的准确性、正确性
- **大价值** Value：大数据具有多维度的价值，可以从中挖掘出巨大的商业价值

#### 5.请简述金融数据的特征。

- **数据众多**：既有交易记录、账单等结构化数据也有视频音频等非结构化数据
- **高时效性**：要求数据在短时间内被快速处理，实时性高。（流数据”特征，需要在短时间内快速处理）
- **可展示**需求强：获取的数据需要以表格、曲线等形式清晰展示，便于进一步分析。
- **逻辑性**：金融数据具有高时序性、相关性，涉及到交易价格等变化具有前后关系的数据。
- **高频性**：股票市场等会在短时间内产生大量交易，需要对实时产生的大量数据进行分析。
- **高度机密性**：金融数据通常包含敏感信息，如个人身份、财务账户等，因此需要高度保密和安全性。
- **高波动性**：金融市场具有高度的波动性，价格和市场情况可以瞬息万变。

# 并行计算

### 内容

#### 分类

- **按数据和指令**处理结构：弗林(Flynn)分类
  - <img src="https://thdlrt.oss-cn-beijing.aliyuncs.com/image-20230918144558018.png" alt="image-20230918144558018" style="zoom:33%;" />
  - ![image.png|425](https://thdlrt.oss-cn-beijing.aliyuncs.com/20240102120408.png)

- 按**并行类型**
  - 位级并行
  - 指令级并行
  - 线程级并行
    - **数据级**并行：一个大的数据块划分为小块，分别由不同的处理器/线程处理
    - **任务级**并行：一个大的计算任务划分为子任务，分别由不同的处理器/线程来处理

- 按**存储访问构架**
  - <img src="https://thdlrt.oss-cn-beijing.aliyuncs.com/image-20230918144733882.png" alt="image-20230918144733882" style="zoom:33%;" />
  - 完全共享、共享&本地、完全本地

#重点
- 按**系统类型**![image.png|75](https://thdlrt.oss-cn-beijing.aliyuncs.com/20240102121512.png)

  - **多核/众核并行计算系统MC**：集成在**一个芯片**上，使用本地缓存加上全局内存，功耗较低。
  - **对称多处理系统 SMP**：用**独立的处理器**和**共享内存**，以**总线结构互联**，运行一个操作系统, 定制成本高,难以扩充,规模较小 (2-8 处理器）
  - **大规模并行处理MPP**：使用**独立处理器，具有独立的内存**，通过专用**内联网连接**，扩展性差，规模中等。
  - **集群Cluster**：使用商品化服务器，通过**网络互联**，可扩展性强，目前最为常用。
  - **网格 Grid**：由**距离较远**的异构服务器构成，较为**松散**，适合**并行度较低**的大规模科学计算。
  - **云 Cloud**：通过**互联网访问计算资源**，资源托管在云服务提供商的远程数据中心

- **按计算特征**
  - **数据密集型**并行计算：数据量极大、但计算相对简单的并行处理，如大规模Web 信息搜索
  - **计算密集型**并行计算：数据量相对不是很大、但计算较为复杂的并行处理，如：3-D建模与渲染，气象预报，科学计算……
  - **数据密集与计算密集混合型**并行计算：兼具数据密集型和计算密集型特征的并行计算，如3D 电影渲染

- 按**并行程序设计模型/方法**
  - **共享内存变量**：多线程共享存储器变量方式进行并行程序设计，会引起数据不一致性，导致数据和资源访问冲突，需要引入同步控制机制；(Pthread，OpenMP)
  - **消息传递方式**：对于分布式内存结构，为了分发数据和收集计算结果，需要在各个计算节点间进行数据通信，最常用的是消息。（MPI）
  - **MapReduce方式**

#### 问题

- 多核/多处理器网络互连结构技术
- 存储访问体系结构
  - 共享存储器体系结构
  - 分布存储体系结构
  - 分布共享存储结构（Cache的一致性问题）
- 分布式数据与文件管理
- 并行计算任务的分解与算法设计
- 并行程序设计模型和方法
  - 共享内存式并行程序设计：为共享内存结构并行计算系统提供的程序设计方法,需提供数据访问同步控制机制(如互斥信号,锁等）
  - 消息传递式并行程序设计：为分布内存结构并行计算系统提供的、以消息传递方式完成节点间数据通信的程序设计方法
  - MapReduce并行程序设计：为解决前两者在并行程序设计上的缺陷，提供一个综合的编程框架，为程序员提供了一种简便易用的并行程序设计方法
- 数据同步访问和通信控制
- 可靠性设计与容错技术（最主要的技术问题）
  - 大型并行计算系统使用大量计算机,因此,节点出错或失效是常态，不能因为一个节点失效导致数据丢失、程序终止或系统崩溃，因此，系统需要具有良好的可靠性设计和有效的失效检测和恢复技
  - 数据失效恢复：大量的数据存储在很多磁盘中，当出现磁盘出错和数据损坏时，需要有良好的数据备份和数据失效恢复机制，保证数据不丢失以及数据的正确性。
  - 系统和任务失效恢复：一个节点失效不能导致系统崩溃，而且要能保证程序的正常运行，因此，需要有很好的失效检测和隔离技术，并进行计算任务的重新调度以保证计算任务正常进行。

#### 系统性能评估

- 并行加速评估公式Amdahl定律：$S=\frac{1}{(1-p)+\frac{P}{N}}$
  - S为加速比、P是程序可以并行的比例、N是处理器的数目
- 一个并行程序可加速程度是有限制的，并非可无限加速，并非处理器越多越好
  - <img src="https://thdlrt.oss-cn-beijing.aliyuncs.com/image-20230918152119513.png" alt="image-20230918152119513" style="zoom:33%;" />

#### MPI

- 功能：用常规语言编程方式，所有节点运行同一个程序，但处理不同的数据
  - 提供点对点通信：同步/异步
    - <img src="https://thdlrt.oss-cn-beijing.aliyuncs.com/image-20230918152620502.png" alt="image-20230918152620502" style="zoom:33%;" />
  - 提供节点集合通信
    - <img src="https://thdlrt.oss-cn-beijing.aliyuncs.com/image-20230918155221476.png" alt="image-20230918155221476" style="zoom:33%;" />
    - <img src="https://thdlrt.oss-cn-beijing.aliyuncs.com/image-20230918155510556.png" alt="image-20230918155510556" style="zoom:33%;" />
  - 提供用户自定义的复合数据类型传输
- <img src="https://thdlrt.oss-cn-beijing.aliyuncs.com/image-20230918152445261.png" alt="image-20230918152445261" style="zoom:33%;" />
  - 通信组：将系统中的处理器划分为不同的通信组
    -  MPI_COMM_WORLD，指明系统中所有的进程都参与通信
    - 一个通信组中的总进程数可以由MPI_Comm_Size调用来确定。
  - 进程标识：为每个进程分配一个进程标识
    - 进程标识号可以由MPI_Comm_Rank调用来确定。
- 特点：
  - 灵活性好，适合于各种计算密集型的并行计算任务
  - 独立于语言的编程规范，可移植性好
  - 有很多开放机构或厂商实现并支持

- 不足：
  - 无良好的数据和任务划分支持
  - 缺少分布文件系统支持分布数据存储管理
  - 通信开销大，当计算问题复杂、节点数量很大时，难以处理，性能大幅下降
  - 无节点失效恢复机制，一旦有节点失效，可能导致计算过程无效
  - 缺少良好的构架支撑，程序员需要考虑以上所有细节问题，程序设计较为复杂


### 题目

##### 1.简述为什么需要并行计算？

- 传统提升计算机性能的手段（提升芯片集成度、改进指令集并行度、提升主频等）几乎耗尽，单核处理器性能提升接近极限。为了进一步提高计算速度，向多核并行计算发展成为发展的必然趋势。
- 涉及到大规模数据和复杂计算的研究及应用领域需要使用并行计算技术，并且金融、医疗、ai等越来越多的领域对大数据处理具有很高的需求。
- 总的来说，单核处理器性能发展的瓶颈，而多核并行计算具有的体积小、功耗低等诸多技术特点和优势

##### 2.并行计算按照系统类型划分，可以分为哪几种？简述每一种系统类型的特点。

- 多核/众核并行计算系统MC：集成在一个芯片上，使用本地缓存加上全局内存，功耗较低。
- 对称多处理系统SMP：多个相同类型的树立起通过总线连接并使用共享内存，扩展性差，规模较小。
- 大规模并行处理MPP：使用独立处理器，具有独立的内存，通过专用内联网连接，扩展性差，规模中等。
- 集群Cluster：使用商品化服务器，通过网络互联，可扩展性强，目前最为常用。
- 网格Grid：有距离较远的异构服务器构成，较为松散，适合并行度较低的大规模科学计算。
- 云Cloud：通过互联网访问计算资源，资源托管在云服务提供商的远程数据中心

##### 3.为什么可靠性设计与容错是并行计算的主要技术问题？

- 使用并行计算需要消耗大量计算资源，并且由于系统设备众多、较为复杂，单个节点的出错和失效不可避免，此如果一个节点的错误会导致整个计算的崩溃，那会导致系统频繁发生故障，计算根本无法进行。
- 并行计算常常用于科学计算、电影渲染等需要长时间持续计算的领域，如频繁因一点小故障而终止那么计算可能根本无法完成
- 并行计算的结果往往是具有重要价值的，错误的结果会带来高昂的代价
- 增加可靠性，避免出错还可以介绍系统维护成本，是提高并行计算竞争力的关键

##### 4.MPI提供哪几种通信方式？简述每种通信方式对应的接口。

- 点对点通信
  - 同步通信
    - 发送：`MPI_Send (buf, count, datatype, dest, tag, comm)`
    - 接收：`MPI_Recv (buf, count, datatype, source, tag, comm, status)`
  - 异步通信
    - 发送：`MPI_ISend (buf, count, datatype, dest, tag, comm, request)`
    - 接收：`MPI_IRecv (buf, count, datatype, source, tag, comm, status, request)`
    - 等待传输完成：`MPI_Wait (request, status)`
    - 检查是否传输完成`MPI_Test (request, flag, status)`
- 节点集合通信
  - 同步（设置同步障碍，使所有进程的执行同时完成，即等待所有进程达到同步点后才能继续执行。）：`int MPI_Barrier(MPI_Comm comm)`
  - 广播发送：`int MPI_Bcast(void *buffer, int count, MPI_Datatype datatype, int root, MPI_Comm comm)`
  - 收集（多个进程的消息以某种次序收集到一个进程）：`int MPI_Gather(void *sendbuf, int sendcount, MPI_Datatype sendtype, void *recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm)`
  - 分散（将一个信息划分为等长的段依次发送给其他进程）：`int MPI_Scatter(void *sendbuf, int sendcount, MPI_Datatype sendtype, void *recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm)`
  - 数据规约（将一组进程的数据按照指定的操作方式规约到一起，并传送给一个进程）：`MPI_Reduce(sendbuf, recvbuf, count, datatype, op, root, comm)`
- 用户自定义的复合数据类型传输

# Google MapReduce

- 主要目标
  - 实现自动并行化计算
    - 为程序员隐藏系统层细节

- MapReduce是面向大规模数据并行处理的
  - 分而治之的策略
  - 基于集群的高性能并行计算平台
  - 并行程序开发与运行框架：可以自动执行并行化，划分数据、任务，分布存储，通讯等工作
  - 并行程序设计模型与方法：函数式语言中的设计思想，用**Map和Reduce**两个函数编程（高层的并行编程抽象模型）实现基本的并行计算任务，提供了完整的并行编程接口，完成大规模数据处理（为大数据处理过程中的两个主要处理操作提供一种**抽象机制**）
- MapReduce提供一个统一的计算框架，可完成：
  - 计算任务的划分和调度
  - 数据的分布存储和划分
  - 处理数据与计算任务的同步
  - 结果数据的收集整理
  - 系统通信、负载平衡、计算性能优化处理
  - 处理系统节点出错检测和失效恢复
- 主要功能
  - 出错处理：由于计算集群以低端商用服务器构成，十分容易出现软件、硬件上的错误，mapreduce可以检测并隔离出错节点，并调度分配新的节点接管出错节点的计算任务。
  - 分布式数据存储与文件管理：把海量数据分布存储在各个节点的本地磁盘上，但保持整个数据在逻辑上成为一个完整的数据文件；并且为了数据的安全，还具有多备份存储能力以及存储容错机制。
  - Combiner和Partitioner：为了减少数据通信开销，需要在将数据发送到reduce处理之前先把具有同样主键的数据合并到一起避免重复传送。map节点输出的中间结果需使用一定的策略进行适当的划分处理，保证相关数据发送到同一个reduce节点。
  - 任务调度：提交的计算作业会被划分为很多个计算任务，任务调度主要负责为这些计算任务分配和调度map及reduce计算节点；并监控节点的执行状况，负责map节点执行的同步控制，以及进行一些计算性能优化处理。
  - 数据/代码互定位：为了减少数据通讯的开销，让一个计算节点尽可能处理其本地磁盘上所分布存储的数据。当无法进行这种本地化数据处理时，再寻找其它可用节点并将数据从网络上传送给该节点处理。
- 设计思想
  - 横向扩展取替向扩展：选用价格便宜、易于扩展的大量低端商用服务器，而非价格昂贵、不易扩展的高端服务器
  - 认为失效是常态：MapReduce集群中使用大量的低端服务器，因此，节点硬件失效和软件出错是常态。任何一个节点失效时，其它节点要能够无缝接管失效节点的计算任务；当失效节点恢复后应能自动无缝加入集群，而不需要管理员人工进行系统配置。要能有效处理失效节点的检测和恢复。
  - 处理向数据迁移：将处理向数据靠拢和迁移，取代把数据传送到处理节点。采用数据/代码互定位的技术方法，计算节点将首先尽量负责计算其本地存储的数据，以发挥数据本地化特点，仅当节点无法处理本地数据时，再采用就近原则寻找其它可用计算节点，并把数据传送到该可用计算节点。
  - 顺序处理数据、避免随机访问数据：大规模数据处理的特点决定了大量的数据记录不可能存放在内存、而只可能放在外存中进行处理，而磁盘的随机访问比顺序访问慢得多。
  - 为应用开发者隐藏系统层细节：MapReduce提供了一种抽象机制将程序员与系统层细节隔离开来，程序员仅需描述需要计算什么，而具体怎么去做就交由系统的执行框架处理。
  - 数据扩展和系统规模扩展：软件算法应当能随着数据规模的扩大而表现出持续的有效性，性能上的下降程度应与数据规模扩大的倍数相当；并且在集群规模上，要求算法的计算性能应能随着节点数的增加保持接近线性程度的增长；
- 不可分拆的计算任务或相互间有**依赖关系**的数据无法进行并行计算，如fib序列；
- 一个大数据若可以分为具有同样计算过程的数据块，并且这些数据块之间**不存在数据依赖关系**，则提高处理速度的最好办法就是并行计算。
  - <img src="https://thdlrt.oss-cn-beijing.aliyuncs.com/image-20230924175455249.png" alt="image-20230924175455249" style="zoom: 33%;" />
  - <img src="https://thdlrt.oss-cn-beijing.aliyuncs.com/image-20230924175519140.png" alt="image-20230924175519140" style="zoom:33%;" />

### Map和Reduce

- Map: 对一组数据元素进行某种重复式的处理
  - $(k1;v1)\to [(k2;v2)]$
  - 输入一个键值对，输出得到的一组中间数据
- Reduce: 对Map的中间结果进行某种进一步的结果整理
  - $(k2;[v2])\to[(k3;v3)]$
  - 将map输出的多组键值对依照键进行合并处理，之后进行进一步整理或处理得到输出结果
- <img src="https://thdlrt.oss-cn-beijing.aliyuncs.com/image-20230924181027913.png" alt="image-20230924181027913" style="zoom:50%;" />
  - 进行reduce处理之前，必须等到所有的map函数做完，因此，在**进入reduce前**需要有一个同步障；这个阶段也负责对map的中间结果数据进行收集整理处理，以便reduce更有效地计算最终结果
- 以多句话的词频统计为例
  - <img src="https://thdlrt.oss-cn-beijing.aliyuncs.com/image-20230924181334816.png" alt="image-20230924181334816" style="zoom:33%;" />
  - <img src="https://thdlrt.oss-cn-beijing.aliyuncs.com/image-20230924181351240.png" alt="image-20230924181351240" style="zoom:33%;" />

### Google MapReduce基本工作原理

- 有一个待处理的大数据，被划分为大小相同的数据块(如64MB),及与此相应的用户作业程序。系统中有一个负责调度的主节点(Master),以及数据Map和Reduce工作节点(Worker)。
- 用户作业程序提交给主节点；主节点为作业程序寻找和配备可用的Map节点，并将程序传送给Map节点；主节点也为作业程序寻找和配备可用的Reduce节点，并将程序传送给Reduce节点
  - <img src="https://thdlrt.oss-cn-beijing.aliyuncs.com/image-20230924205610507.png" alt="image-20230924205610507" style="zoom:33%;" />
- 主节点启动每个Map节点执行程序，每个Map节点尽可能读取本地或本机架的数据进行计算;没个Map节点处理读取的数据块,并做一些数据整理工作(combining, sorting等)并将中间结果存放在本地；同时通知主节点计算任务完成并告知中间结果数据存储位置。
  - <img src="https://thdlrt.oss-cn-beijing.aliyuncs.com/image-20230924205727637.png" alt="image-20230924205727637" style="zoom:33%;" />
- 主节点等所有Map节点计算完成后，开始启动Reduce节点运行；Reduce节点从主节点所掌握的中间结果数据位置信息，远程读取这些数据；Reduce节点计算结果汇总输出到一个结果文件即获得整个处理结果
  - <img src="https://thdlrt.oss-cn-beijing.aliyuncs.com/image-20230924205836781.png" alt="image-20230924205836781" style="zoom:33%;" />

#### 失效处理

- 主节点失效：节点中会周期性地设置检查点检查整个计算作业的执行情况，一旦某个任务失效，可以从最近有效的检查点开始重新执行，避免从头开始计算的时间浪费。
- 工作节点失效：主节点会周期性地给工作节点发送检测命令，如果工作节点没有回应，这认为该工作节点失效，主节点将终止该工作节点的任务并把失效的任务重新调度到其它工作节点上重新执行。

#### 优化

- 带宽优化
  - 大量的键值对数据在传送给Reduce节点时会引起较大的通信带宽开销：每个Map节点处理完成的中间键值对将由combiner做一个合并压缩，即把那些键名相同的键值对归并为一个键名下的一组数值。（reduce前的结果整理）
- 计算优化
  - Reduce节点必须要等到所有Map节点计算结束才能开始执行：把一个Map计算任务让**多个**Map节点同时做，取**最快完成者**的计算结果
- 数据分区解决数据相关性
  - 一个Reduce节点上的计算数据可能会来自多个Map节点，因此，为了在进入Reduce节点计算之前，需要把属于一个Reduce节点的数据**归并**到一起。在Map阶段进行了Combining以后，可以根据一定的策略对Map输出的中间结果进行分区，这样即可解决以上数据相关性问题避免Reduce计算过程中的数据通信。
  - 有一个巨大的数组，其最终结果需要排序，每个Map节点数据处理好后，为了避免在每个Reduce节点本地排序完成后还需要进行全局排序，我们可以使用一个分区策略如:(d%R)，d为数据大小，R为Reduce节点的个数，则可根据数据的大小将其划分到**指定数据范围**的Reduce节点上，每个Reduce将本地数据排好序后即为最终结果。

### 分布式文件系统Google GFS

- Google GFS是一个基于分布式集群的大型分布式文件系统，为MapReduce计算框架提供数据存储和数据可靠性支撑；数据存储在物理上分布的每个节点上，但通过GFS将整个数据形成一个逻辑上整体的文件。
- 设计原则
  - 廉价本地磁盘分布存储：各节点本地分布式存储数据，不需要采用价格较贵的集中式磁盘阵列，容量可随节点数增加自动增加
  - 多数据自动备份解决可靠性：采用廉价的普通磁盘，把磁盘数据出错视为常态，用自动多数据备份存储解决数据存储可靠性问题
  - 为上层的MapReduce计算框架提供支撑：GFS作为向上层MapReduce执行框架的底层数据存储支撑，负责处理所有的数据自动存储和容错处理，因而上层框架不需要考虑底层的数据存储和数据容错问题

#### 基本架构和工作原理

- <img src="https://thdlrt.oss-cn-beijing.aliyuncs.com/image-20230924211347529.png" alt="image-20230924211347529" style="zoom:33%;" />

- GFS Master

  - Master上保存了GFS文件系统的三种元数据 ：
    - 命名空间即整个分布式文件系统的目录结构
    - Chunk**与文件名的映射表**
    - Chunk副本的位置信息，每一个Chunk默认有3个副本
  - 前两种元数据可通过操作日志提供容错处理能力；
  - 第3个元数据直接保存在ChunkServer上，Master启动或Chunk Server注册时自动完成在Chunk Server上元数据的生成；
  - 当Master失效时，只要ChunkServer数据保存完好，可迅速恢复Master上的元数据。

- GFS ChunkServer（用来保存大量实际数据的数据服务器）

  - GFS中每个数据块划分缺省为64MB，每个数据块会分别在3个(缺省情况下)不同的地方复制副本；对每一个数据块，仅当3个副本都更新成功时，才认为数据保存成功。
  - 当某个副本失效时，Master会自动将正确的副本数据进行复制以保证足够的副本数；
  - GFS上存储的数据块副本，在物理上以一个本地的Linux操作系统的文件形式存储，每一个数据块再划分为64KB的子块，每个子块有一个32位的校验和，读数据时会检查校验和以保证使用未失效的数据。

- 数据访问工作过程

  - 在程序运行前，数据已经存储在GFS文件系统中；程序运行时应用程序会告诉GFS Server所要访问的文件名或者数据块索引是什么
  -  GFS Server根据文件名和数据块索引在其文件目录空间中查找和定位该文件或数据块，找出数据块**具体**在哪些ChunkServer上；将这些位置信息回送给应用程序
    - <img src="https://thdlrt.oss-cn-beijing.aliyuncs.com/image-20230924212613064.png" alt="image-20230924212613064" style="zoom:33%;" />
  - 应用程序根据GFS Server返回的具体Chunk数据块位置信息，直接访问相应的ChunkServer，直接读取指定位置的数据进行计算处理
    - <img src="https://thdlrt.oss-cn-beijing.aliyuncs.com/image-20230924212717535.png" alt="image-20230924212717535" style="zoom:33%;" />

  - 特点：应用程序访问具体数据时不需要经过GFS Master，因此，避免了Master成为访问瓶颈
  - 并发访问：由于一个大数据会存储在不同的ChunkServer中，应用程序可实现并发访问

- GFS的系统管理技术
  - 大规模集群安装技术：在一个成千上万个节点的集群上迅速部署GFS，升级管理和维护等
  - 故障检测技术：GFS是构建在不可靠的廉价计算机之上的文件系统，节点数多，故障频繁，如何快速检测、定位、恢复或隔离故障节点
  - 节点动态加入技术：当新的节点加入时，需要能自动安装和部署GFS
  - 节能技术：服务器的耗电成本大于购买成本，Google为每个节点服务器配置了蓄电池替代UPS，大大节省了能耗。

### 分布式结构化数据表BigTable

- 在GFS之上的一个**结构化数据**存储和访问管理系统BigTable，为应用程序提供比单纯的文件系统更方便、更高层的数据操作能力。
- 主要解决一些大型媒体数据（Web文档、图片等）的结构化存储问题。其结构化粒度没有那么高，也没有事务处理等能力，因此，它并不是真正意义上的数据库。

#### 设计目标

- 需要存储多种数据：如URL，网页，图片，地图数据，email，用户的个性化设置等
- 海量的服务请求：Google是目前世界上最繁忙的系统，因此，需要有高性能的请求和数据处理能力
- 商用数据库无法适用：在如此庞大的分布集群上难以有效部署商用数据库系统，且其难以承受如此巨量的数据存储和操作需求
- 广泛的适用性：可满足对不同类型数据的存储和操作需求
- 很强的可扩展性：根据需要可随时自动加入或撤销服务器节点
- 高吞吐量数据访问：提供P级数据存储能力
- 高可用性和容错性：保证系统在各种情况下都能正常运转，服务不中断
- 自动管理能力：自动加入和撤销服务器，自动负载平衡
- 简单性：系统设计尽量简单以减少复杂性和出错率

#### 数据模型

- BigTable主要是一个分布式多维表：通过一个行关键字、一个列关键字、一个时间戳进行索引和查询定位。
  -  (row:string, column:string,time:int64)-> 结果数据字节串（cell）
  - 支持查询、插入和删除操作
- BigTable对存储在表中的数据不做任何解释，一律视为字节串，具体数据结构的实现由用户自行定义。
- 特征：
  - 稀疏：这表示Bigtable的表中的数据通常是非常稀疏的，也就是说，表中只有一小部分单元格包含有效数据，而绝大多数单元格为空或未定义。这种稀疏性允许Bigtable有效地处理大规模数据而不浪费存储空间。
  - 分布式：Bigtable是一个分布式数据库系统，它的表数据分布在多台物理服务器上。
  - 持久化：Bigtable中的数据是持久化存储的，这意味着数据不会因为服务器故障或系统重启而丢失。数据会被持续存储并可靠地保留。
  - 多维度：Bigtable的表是多维度的，这表示数据可以根据多个维度进行组织和检索。
  - 有序映射：Bigtable中的数据可以被视为按照行键和列限定符**进行排序**的映射（Map），这使得数据的检索和范围查询变得非常高效。
- 存储格式
  - <img src="https://thdlrt.oss-cn-beijing.aliyuncs.com/image-20230924214347580.png" alt="image-20230924214347580" style="zoom:33%;" />
  - 行：大小不超过64KB的任意字符串。表中的数据都是根据**行关键字**进行排序的**。
    - URL地址倒排好处是：同一地址的网页将被存储在表中**连续**的位置，便于查找；倒排便于数据压缩，可大幅提高数据压缩率
  - 子表：一个大表可能太大，不利于存储管理，将在水平方向上被分为
    多个子表
  - 列<img src="https://thdlrt.oss-cn-beijing.aliyuncs.com/image-20230924214712156.png" alt="image-20230924214712156" style="zoom:33%;" />
    - 多个列构成一个列族，族中的数据属于同一类别，一个列族下的数据会被压缩在一起存放，列族是访问控制的单位。数据库管理员或数据所有者可以为不同的列族设置不同的访问权限和安全策略。
    - 查询具体一个列的列键可以由两部分组成（族名：列名）。列族用于将数据分类，而列限定符用于唯一标识特定列族中的数据单元格。通过这种方式，可以方便地对数据库中的数据进行组织和检索。同时，不同的列族可以拥有不同的列限定符，从而允许更灵活的数据模型和查询。
    - content、anchor都是族名；而cnnsi.com和my.look.ca则是anchor族中
      的列名。
  - 时间戳：很多时候同一个URL的网页会不断更新，而Google需要保存不同时间的网页数据，因此需要使用时间戳来加以区分。（t3 t5 t6）
    - 可以保留最近的n个版本数据/保留限定时间内的所有不同版本数据

#### 基本架构

- <img src="https://thdlrt.oss-cn-beijing.aliyuncs.com/image-20230924215701349.png" alt="image-20230924215701349" style="zoom:50%;" />
- 主服务器
  - 新子表分配：当一个新子表产生时，主服务器通过加载命令将其分配给一个空间足够大的子表服务器；创建新表、表合并及较大子表的分裂都会产生新的子表。
  - 子表监控：通过Chubby完成。所有子表服务器基本信息被保存在Chubby中的服务器目录中主服务器检测这个目录可获取最新子表服务器的状态信息。当子表服务器出现故障，主服务器将终止该子表服务器，并将其上的全部子表数据移动到其它子表服务器。
  - 负载均衡：当主服务器发现某个子表服务器负载过重时，将自动对其进行负载均衡操作。
- 子表服务器
  - BigTable中的数据都以子表形式保存在子表服务器上，客户端程序也直接和子表服务器通信。
  - 分配：当一个新子表产生，子表服务器的主要问题包括子表的定位、分配、及子表数据的最终存储。
  - 子表数据结构<img src="https://thdlrt.oss-cn-beijing.aliyuncs.com/image-20230924220106276.png" alt="image-20230924220106276" style="zoom:25%;" />
    - 子表是整个大表的多行数据划分后构成。而一个子表服务器上的子表将**进一步**由很多个SSTable构成，每个SSTable构成最终的在底层GFS中的存储单位。
    - 一个SSTable还可以为不同的子表所共享，以避免同样数据的重复存储。
    - SSTable是BigTable内部的基本存储结构，以GFS文件形式存储在GFS文件系统中。一个SSTable实际上对应于GFS中的一个64MB的数据块
    - SSTable中的数据进一步划分为64KB的子块，因此一个SSTable可以有多达1千个这样的子块。为了维护这些子块的位置信息，需要使用一个Index索引。

- <img src="https://thdlrt.oss-cn-beijing.aliyuncs.com/image-20230924220206684.png" alt="image-20230924220206684" style="zoom:50%;" />

