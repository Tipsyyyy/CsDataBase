{"path":"课程/金融大数据/课件/10 MapReduce数据挖掘基础算法 (I).pdf","text":"K - M ea n s 聚类算法 M a p R ed u c e 数据挖掘基础算法 (I ) 摘要 ¨ 为什么选择数据挖掘作为并行计算的研究点 ¨ K - M ea n s 聚类算法介绍 ¨ K - M ea n s 算法为什么适合使用并行方法 ¨ 基于 M a p R ed u c e 的 K - M ea n s 并行算法 ¨ 问题讨论 2 数据挖掘 ¨ 定义：数据挖掘是通过对 大规模观测数据集 的分析，寻找确信的关 系，并将数据以一种可理解的、且利于使用的新颖方式概括数据的 方法。 ¨ 数据挖掘的特征之一：海量数据 ¤ Sm a l l d a t a d oes n ot r eq u i r e d a t a m i n i n g , l a r g e d a t a c a u s es p r ob l em s . ¨ 可见，数据挖掘是并行计算中值得研究的一个领域。 3 数据挖掘 ¨ 研究发现大数据隐含着更为准确的事实 4 2001 年微软研究院的 Ba n k o an d Br i l i 等研究发 现数据越大，机器学习 的精度越高；当数据不 断增长时，不同算法的 分类精度趋向于相同！ M. Ba n k o an d E . Br i l i (2001). Sc a l i ng t o ve r y ve r y l a r g e c o r p o r a f o r n a t u r a l la n g u a g e d is a m b ig u a tio n . ACL 2 0 0 1 . 数据挖掘 ¨ 研究发现大数据隐含着更为准确的事实 5 2007 年 G oog l e 公司的 Br a n t s 等基于 M a p R ed u c e 研究了一个 2 万亿单词训 练数据集的语言模型， 发现大数据集上的简单 算法能比小数据集上的 复杂算法产生更好的结 果！ T. Br a n t s , A .C . P op a t , et a l . ( 2 0 0 7 ) . La r g e l a n g u a g e m o d e l s i n m a c h i n e tr a n s la tio n . In E MN L P - C oN l l 2007. 聚类的应用领域 ¨ 市场营销 ¤ 给定一个很大的顾客交易集，找出有类似购买行为的顾客分组 ¨ 文档分类 ¤ 对 We b 日志数据聚类，发现有类似访问模式的分组 ¨ 保险 ¤ 通过识别可能的欺诈行为找出平均索赔支出很高的车险投保人群 ¨ … 6 聚类过程 ¨ 定义：将给定的多个对象分成若干组， 组内的各个对象是相似的， 组间的对象是不相似的 。进行划分的过程就是聚类过程，划分后的 组称为 簇（ c l u s t er ） 。 ¨ 几种聚类方法： Ø 基于划分的方法； Ø 基于层次的方法； Ø 基于密度的方法； Ø ...... 7 点、空间和距离 ¨ 点集是一种适合于聚类的数据集，每个点都是某空间下的对象。 ¤ 欧式空间下的点就是实数向量； ¤ 向量的长度是空间的维数； ¤ 向量的分量通常称为所表示点的坐标。 ¨ 能够进行聚类的所有空间下都有一个距离测度，即给出空间下任意 两点的距离。 ¤ 距离永远非负，只有点到自身的距离为 0 ； ¤ 距离具有对称性； ¤ 距离遵守三角不等式。 8 数值类型 ¨ 数据点的类型可分为： Ø 欧氏（ E u c l i d ea n ）空间：空间中的点的平均总是存在，并且也是空间 中的一个点。一般用欧几里得距离来衡量两个点之间的距离。 Ø 非欧空间： Ja c c a r d 距离， C os i n e 距离， Ed i t 编辑距离等多种距离衡量方 法。 ¨ 这二者在数据的表示以及处理上有较大的不同： Ø 怎样来表示 c l u s t er ？ Ø 怎样来计算相似度 ？ 9 C l u s t er 的表示 ¨ 欧氏空间： Ø 取各个数据点的平均值（ c en t r oi d ） ¨ 非欧空间： Ø 取某个处于最中间的点 Ø 取若干个最具代表性的点（ c l u s t r oi d ） Ø ...... 10 相似度 （距离） 的计算 ¨ 欧氏空间： 可以有较为简单的方法 ¨ 非欧氏空间： 通常不能直接进行简单的数字计算 Ø Ja c c a r d 距离 : 两个集合中不同元素占所有元素的比例 Ø C os i n e 距离：两个向量的夹角大小 Ø Ed it 距离：适合于 st ri n g 类型的数据 Ø Ham m i n g 距离：两个向量中不同分量的个数 11 基于划分（ p a r t i t i on i n g ）的聚类方法 ¨ 给定 N 个对象，构造 K 个分组，每个分组就代表一个聚类。 ¨ 这 K 个分组满足以下条件： Ø 每个分组至少包含一个对象； Ø 每个对象属于且仅属于一个分组。 ¨ K - M ea n s 算法 是最常见和典型的基于划分的聚类方法 12 注：本节课只讨论 欧氏空间 里的 K - M ea n s 聚类方法 K - M ea n s 算法 13 过程示例（ 1 ） 14 初始数据 K = 2 选择初始中心 ----------------------------------------------------------------------- ----------------------------------------------------------------------- 第 1 次聚类：计算距离 + + 过程示例（ 2 ） 15 第 1 次聚类：归类各点 ----------------------------------------------------------------------- 重新计算聚类中心 + + 过程示例（ 3 ） 16 第 2 次聚类：计算距离 ----------------------------------------------------------------------- 第 2 次聚类：归类各点 + + + + 聚类无变化，迭代终止 K - M ea n s 是个不断迭代的过程 ¨ 第 i 轮迭代： Ø 生成新的 c l u s t er s ，并计算 c l u s t er c en t er s ¨ 第 i+ 1 轮迭代： Ø 根据第 i 轮迭代中生成的 c l u s t er s 和计算出的 c l u s t er c en t er s ，进行新一轮的聚类 ² 如此不断迭代直到满足终止条件 17 K - M ea n s 算法的局限性 ¨ 对初始 c l u s t er c en t er s 的选取会影响到最终的聚类结果 ¨ 由此带来的结果是： 能得到局部最优解，不保证得到全局最优解 ¨ 相似度计算和比较时的计算量较大 18 K - M ea n s 计算性能的瓶颈 ¨ 如果样本数据有 n 个，预期生成 k 个 c l u s t er ，则 K - M ea n s 算法 t 次迭代过 程的时间复杂度为 O(n * k * t) ，需要计算 n * t * k 次相似度 ¨ 如果能够 将各个点到 c l u s t er c en t er 相似度的计算工作分摊到不同的 机器上并行地计算 ，则能够减少计算时间 ¨ 利用 M a p R ed u c e 将 K - M ea n s 聚类过程并行化 19 考虑数据相关度 ¨ 在进行 K - M ea n s 聚类中，在处理每一个数据点时 ¨ 只需要知道： ¤ 各个 c l u s t er 的中心信息 ¨ 不需要知道： ¤ 关于其他数据点的任何信息 ² 所以， 如果涉及到全局信息，只需要知道关于各个 c l u s t er c en t er 的信息即 可 20 并行化改造的出发点 ¨ 将所有的数据分布到不同的 n od e 上，每个 n od e 只对自己的数据进行计算 ¨ 每个 n od e 能够读取上一次迭代生成的 c l u s t er c en t er s ，并判断自己的各个数据点 应该属于哪一个 c l u s t er ¨ 每个 n od e 在每次迭代中根据自己的数据点计算出相关数据 ¨ 综合每个节点计算出的相关数据，计算出最终的实际 c l u s t er c en t er s 21 需要全局共享的数据 ¨ 每一个节点需要访问如下的全局文件 C 当前的迭代计数 C K 个 如下结构 c l u s t er i d c l u s t er c en t er 属于该 c l u s t er c en t er 的数据点的个数 ¨ 这是 唯一的全局文件 22 架构图 23 Al l D a t a Ma p p e r 1 Ma p p e r 2 Ma p p e r 3 聚 类 中 心 信 息 Sh u f f l e a n d So r t Re d u c e r 1 Re d u c e r 2 Map 阶段的处理 ¨ M a p p er 伪代码 cl ass M apper se t u p ( …) { 读出全局的聚类中心数据 à Ce n t e r s } m ap ( ke y , p ) // p 为一个数据点 { mi n D i s = Do u b le . M AX VA L U E ; i n d e x = - 1; fo r i =0 t o C e n t e r s. l e n g t h { d i s= C o m p u t e D i st ( p , C e n t e r s[ i ]); i f d i s < mi n D i s { mi n D i s = d i s; i n d e x = i ; } } e m i t ( C e n t e r s[ i ]. C l u st e r I D , (p ,1 )); } 24 思考题： 在 ma p 中，最后一行 em i t ( C en t er s [ i ]. C l u s t er I D , ( p ,1 ) ) 语句中， 为什么输出值必须是 (p , 1) 而不能是 p ？如果写成 p 会带来什 么问题？ Map 阶段的处理 ¨ C om b i n er 伪代码 cl ass C om bi ner r e d u ce ( C l u st e r I D , [( p 1 ,1 ) , ( p 2 ,1 ) , … ] ) { pm = 0. 0 ； n = 数据点列表 [( p 1 ,1 ) , ( p 2 ,1 ) , … ] 中数据点 的总个数 ; fo r i =0 t o n pm + = p[ i ]; pm = pm / n; / / 求得这些数据点的平均值 em i t ( C l u st e r I D , (p m, n )); } 25 R ed u c e 阶段的处理 ¨ R ed u c er 伪代码 cl ass R educer r e d u ce ( C l u st e r I D , va l u e = [( p m 1 ,n 1 ) ,( p m 2 ,n 2 ) … ] ) { pm = 0. 0 ； n= 0; k = 数据点列表 中数据项 的总个数 ; fo r i = 0 t o k { p m += p m [ i ] * n[ i ]; n + = n [ i ]; } pm = pm / n; / / 求得所有属于 C l u st e r I D 的数据点的均值 em i t ( C l u st e r I D , ( pm , n )); // 输出新的聚类中心的数据值 } 26 用不用 C om b i n er 仅仅会影响性能，不能改变计算结果。因此， C om b i n er 输 出时不允许改变 Map 输出键值对中 Va l u e 的格式和类型，否则会出错 数据举例说明 ¨ 令 k = 2 ，欲生成 c l u s t er - 0 和 c l u s t er - 1 ¨ 随机选取 A(1, 1) 作为 c l u s t er - 0 的中心， C(4, 3) 作为 c l u s t er - 1 的中心 ¨ 假定将所有数据分布到 2 个节点 n od e - 0 和 n od e - 1 上 , 即 n od e - 0 ： A( 1 , 1 ) 和 C( 4 , 3 ) n od e - 1 ： B( 2 , 1 ) 和 D( 5 , 4 ) 27 准备 开始 K - M ea n s 聚类 ¨ 在开始之前，首先创建一个如前所述的全局文件 28 Map 阶段 ¨ 每个节点读取全局文件，以获得上一轮迭代生成的 c l u s t er c en t er s 等 信息 ¨ 计算本节点上的每一个点到各个 c l u s t er c en t er 的距离，选择距离最 近的 c l u s t er ¨ 为每一个数据点，发射键值对 < C l u t er I D , ( 数据点自身 , 1 ) > 29 Map ： I t er a t i on 1 ¨ 计算各个数据点到各个 c l u s t er 的距离，假定是如下的结果 30 N od e - 0 N od e - 1 n od e - 0 输出： <c l u st e r - 0, [ A ( 1,1) ,1] > <c l u st e r - 1, [ C( 4,3) ,1] > n od e - 1 输出： <c l u st e r - 0, [ B ( 2,1) ,1] > <c l u st e r - 1, [ D( 5,4) ,1] > C om b i n e 阶段 ¨ 利用 c om b i n er 减少 ma p 阶段产生的大量的 C l u s t er I D 相同的键值对 < C l u s t er I D , ( p, k )> ¨ C om b i n er 计算要 em i t 的所有数据点的均值，以及这些数据点的个数 ¨ 然后，为每一个 c l u s t er 发射新的键值对 < C l u s t er I D , ( pm , n )> 31 C om b i n e ： I t er a t i on 1 ¨ Map 的输出（即 C om b i n er 的输入）： ¨ C om b i n er 的输出 ¤ < C l u s t er I D , ( p m , n ) > 32 n od e - 0 发射： <c l u st e r - 0, [ A ( 1,1) ,1] > <c l u st e r - 1, [ C( 4,3) ,1] > n od e - 1 发射： <c l u st e r - 0, [ B ( 2,1) ,1] > <c l u st e r - 1, [ D( 5,4) ,1] > n od e - 0 发射： <c l u st e r - 0, [ ( 1,1) ,1] > <c l u st e r - 1, [ ( 4,3) ] ,1] > n od e - 1 发射： <c l u st e r - 0, [ ( 2,1) ,1] > <c l u st e r - 1, [ ( 5,4) ,1] > R ed u c e 阶段 ¨ 由于 ma p 阶段 em i t 的 ke y 是 C l u s t er I D ，所以 每个 c l u s t er 的全部数据将 被发送同一个 r ed u c er ，包括： ¤ 该 cl u s t er 的 ID ¤ 该 cl u s t er 的数据点的均值，及对应于该均值的数据点的个数 ¨ 然后经计算后输出 C 当前的迭代计数 C C l u s t er I D C 新的 C l u s t er C en t er C 属于该 C l u s t er C en t er 的数据点的个数 33 R ed u c e ： I t er a t i on 1 ¨ 在上一阶段， C om b i n er 的输出 ¨ 两个 r ed u c er 分别收到 34 n od e - 0 发射： <c l u st e r - 0, [ ( 1,1) ,1] > <c l u st e r - 1, [ ( 4,3) ,1] > n od e - 1 发射： <c l u st e r - 0, [ ( 2,1) ,1] > <c l u st e r - 1, [ ( 5,4) ,1] > R ed u c er - 0 ： <c l u st e r - 0, [ ( 1,1) ,1] > <c l u st e r - 0, [ ( 2,1) ,1] > R ed u c er - 1 ： <c l u st e r - 1, [ ( 4,3) ,1] > <c l u st e r - 1, [ ( 5,4) ,1] > R ed u c e ： I t er a t i on 1 ¨ 计算可得  c l u s t er - 0 的中心 = ( ( 1* 1+ 2* 1) / ( 1+ 1) , ( 1* 1+ 1* 1) / ( 1+ 1) ) = ( 1. 5, 1)  c l u s t er - 1 的中心 = ( ( 4* 1+ 5* 1) / ( 1+ 1) , ( 3* 1+ 4* 1) / ( 1+ 1) ) = ( 4. 5, 3. 5) 35 R ed u c e ： I t er a t i on 1 ¨ R ed u c er 输出 36 (1 . 5 , 1 ) (4 . 5 , 3 . 5 ) 2 2 第 1 轮迭代结束 ¨ 下面开始第 2 轮迭代，此时，全局文件已经更新为： 37 (1 . 5 , 1 ) (4 . 5 , 3 . 5 ) 2 2 终止迭代 ¨ 在第 i 次迭代后，已经生成了 K 个聚类。如果满足了终止条件，即可 停止迭代，输出 K 个聚类 ¨ 终止条件： Ø 设定迭代次数 ; Ø 均方差的变化（非充分条件） Ø 每个点固定地属于某个聚类 Ø 其他设定条件 . . . . . . Ø 与具体的应用高度相关 38 T es t C a s e ¨ 参数 K= 3, T = 3 ¤ J ob Li s t ¤ ou t p u t 39 算法设计实现小结 ¨ 利用 M a p R ed u c e 来并行化 K - M ea n s 聚类过程是 可行 的 ¨ 每个节点计算一部分数据的归属，从而实现并行 ¨ 数据间是无关的，但是数据和聚类中心是相关的 ，因此需要全局文件，但不构 成性能瓶颈 ¨ 没有因为并行而降低了算法的精确度（每一个点均保证与每一个 c l u s t er c en t er 进行了比较） 40 O p en Pr ob l em N et F l i x 百万美元 大奖赛 N et f l i x 公司 ¨ 美国的一家电影在线租赁公司 ¨ 拥有大量用户的影评记录 ¨ 影片推荐系统 基于这些影评记录 ¨ 能够将推荐正确率提高 10% 者，将获得 100 万美元的奖励 竞赛数据 ¨ 训练数据集：由 N et f l i x. c om 提供的由来自超过 480K 名随机选择的匿 名用户对接近 18K 部电影的影评（超过 100M 条） ¨ 影评的等级范围：从 ★ 到 ★★★★★ ¨ 测试数据集：包含超过 2. 8M 条记录。每条记录包含用户 ID 、电影 ID 、 日期、影评等级，但是影评信息是空白。测试数据集是训练数据集 的一个子集。 竞赛数据  有 17770 个影片的影评文件，每个文件代表一部影片，描述了观 众对于该影片的评价  评价的指数从 ★ 到 ★★★★★ ，每个影评文件的格式如：  第一行： m ov i eI D  其余每行： u s er I D , r a t ing , d a t e  两个影评文件中的观众 id 可能有相同的，也可能不同。 对提交算法的要求 ¨ 对于测试数据集中的任一个 < 用户 ID , 电影 ID > 对，算法必须能够 预测出该用户对该电影的影评，即给出影评的星级 (1 - 5 级 ) 。 ¨ 算法的评价标准 ¤ 测试集将被划分为不交的两个子集（划分的方法是保密的），对这两个子集中的每一个预 测影评， N et f l i x. c om 将计算其与对应的真实影评的 均方根误差 ( RM SE , R oot M ea n Sq u a r ed E r r or ) ，计算结果精确到 0. 0001 。 ¤ （这里 n 是测量次数， di 是一组测量值与平均值的误差。） 1 1 2 - å = n d n i i 竞赛结果 ¨ C i n em a t c h 影片推荐引擎形成了在前述训练数据集上的预测算法， 该预测算法对测试数据集所做的预测的 RM SE 为 0. 9525 。 ¨ 要想拿到百万美金大奖，参赛者至少将预测的精确度提高 10% ， 所以，算法的预测结果的 RM SE 必须不大于 0. 8572 。 ¨ Bel l K or ’ s Pr a g m a t i c C h a os 为本次竞赛的领先团队，他们所提交的 算法的 RM SE 为 0. 8567 。 N et f l i x 中的聚类问题 ¨ 目的： 根据观众的评价对这 17770 部影片进行数据挖掘，输出约 400 个聚类 ，使得每个聚类中的影片是相似的 ¨ 考虑： ¨ 1. 怎样定义及计算这类聚类问题中的相似度 ¨ 2. 怎样表示一个聚类（或聚类中心） ¨ 3. 对于高维数据怎样预处理 ¨ 4. 怎样减少高维数据的计算量 参考文献 ¨ W e b M i n i n g – I I : P ar al l e l i z i n g K - M e an s C l u s t e r i n g w i t h M apR e du c e . By T u s h a r D es h p a n d e , T ej a s V or a . ¨ M i n i n g of M as s i v e D a t as e t s . By An a n d Ra j a r a m a n , J e ﬀ r e y D . U l l m a n . ¨ Da ta - I n t e n s i v e T e xt Pr oc e s s i n g w i t h M apR e du c e . By J i m m y L i n a n d C h r i s D y er ¨ Da ta A l g or i t h m s – R e c i pe s f or Sc al i n g U p w i t h H adoop an d Spar k . By M a h m ou d Pa r s i a n TH A NK YO U","libVersion":"0.2.4","langs":""}