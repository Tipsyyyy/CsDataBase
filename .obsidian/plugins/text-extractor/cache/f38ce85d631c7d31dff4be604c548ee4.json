{"path":"课程/金融大数据/课件/04 Google MapReduce基本架构.pdf","text":"G oog l e M a p R ed u c e 基本架构 G oog l e 的三驾马车 ¨ SO SP2 0 0 3 The G oog l e Fi l e Sys t em ¨ O SD I 2 0 0 4 M a p R ed u c e: s i m p l i f i ed dat a p r oc es s i n g on la r g e c l u s t er s ¨ O SD I 2 0 0 6 Bi g t a b l e : a d i s t r i b u t ed s t or a g e s ys t em f or s t r u c t u r ed dat a 2 GF S M a p R ed u c e Bi g t a b l e 摘要 ¨ G oog l e M a p R ed u c e 的基本工作原理 ¨ 分布式文件系统 GF S 的基本工作原理 ¨ 分布式结构化数据表 Bi g T a b l e 的基本工作原理 摘要 ¨ G oog l e M a p R ed u c e 的基本工作原理 ¨ 分布式文件系统 GF S 的基本工作原理 ¨ 分布式结构化数据表 Bi g T a b l e 的基本工作原理 G oog l e M a p R ed u c e 并行处理的基本过程 5 Ci t e f r o m D e a n a n d Gh e m a w a t (O S D I 2 0 0 4 ) 1. 有一个待处理的大 数据，被划分为大 小相同的数据块 ( 如 64M B ) , 及与此相应 的用户作业程序 2. 系统中有一个负责调 度的主节点 (M a s ter ), 以 及数据 Ma p 和 Re du c e 工 作节点 (W o r k er ) 6 Ci t e f r o m D e a n a n d Gh e m a w a t (O S D I 2 0 0 4 ) 3. 用户作业程序提交 给主节点 4. 主节点为作业程序 寻找和配备可用的 Ma p 节点，并将程 序传送给 Ma p 节点 5. 主节点也为作业程序 寻找和配备可用的 Re du c e 节点，并将程 序传送给 Re du c e 节点 G oog l e M a p R ed u c e 并行处理的基本过程 7 Ci t e f r o m D e a n a n d Gh e m a w a t (O S D I 2 0 0 4 ) 6. 主节点启动每个 Ma p 节点执行程序， 每个 Ma p 节点尽可 能读取本地或本机 架的数据进行计算 7. 每个 Ma p 节点处理读取的 数据块 , 并做一些数据整理 工作 ( c o m b i n i n g , s o r t i n g 等 ) 并将中间结果存放在本地； 同时通知主节点计算任务 完成并告知中间结果数据 存储位置 G oog l e M a p R ed u c e 并行处理的基本过程 8 Ci t e f r o m D e a n a n d Gh e m a w a t (O S D I 2 0 0 4 ) 8. 主节点等所有 Ma p 节 点计算完成后，开始 启动 Re du c e 节点运行； Re du c e 节点从主节点 所掌握的中间结果数 据位置信息，远程读 取这些数据 9 . Re du c e 节点计算结果汇 总输出到一个结果文件 即获得整个处理结果 G oog l e M a p R ed u c e 并行处理的基本过程 9 Ci t e f r o m D e a n a n d Gh e m a w a t (O S D I 2 0 0 4 ) 完整计算过程 G oog l e M a p R ed u c e 并行处理的基本过程失效处理 ¨ 主节点失效 ¤ 主节点中会周期性地设置检查点 ( c h ec k p oi n t ) ，检查整个计算作业的执行情况，一旦某 个任务失效，可以从最近有效的检查点开始重新执行，避免从头开始计算的时间浪费。 ¤ 如果只有一个 M a s t er ，它不太可能失败；因此，如果 M a s t er 失败，将中止 M a p R ed u c e 计 算。 ¨ 工作节点失效 ¤ 工作节点失效是很普遍发生的，主节点会周期性地给工作节点发送检测命令，如果工 作节点没有回应，这认为该工作节点失效，主节点将终止该工作节点的任务并把失效 的任务重新调度到其它工作节点上重新执行。 10 带宽优化 ¨ 问题 ¤ 大量的键值对数据在传送给 R ed u c e 节点时会引起较大的通信带宽开销。 ¨ 解决方案 ¤ 每个 Map 节点处理完成的中间键值对将由 c om b i n er 做一个合并压缩，即把那 些键名相同的键值对归并为一个键名下的一组数值。 11 (g o o d , 1 ) (w e a th e r , 1 ) (i s , 1 ) (g o o d , 1 ) (g o o d , 2 ) (w e a th e r , 1 ) (i s , 1 ) c om b i n er 计算优化 ¨ 问题 ¤ R ed u c e 节点必须要等到所有 Map 节点计算结束才能开始执行，因此，如果有一个计算量大、 或者由于某个问题导致很慢结束的 Map 节点，则会成为严重的“拖后腿者”。 ¨ 解决方案 ¤ 把一个 Map 计算任务让多个 Map 节点同时做，取最快完成者的计算结果。 12 根据 G oog l e 的测试，使用了这个冗余 Map 节 点计算方法以后，计算任务性能提高 40% 多！ 用数据分区解决数据相关性问题 ¨ 问题 ¤ 一个 R ed u c e 节点上的计算数据可能会来自多个 Map 节点，因此，为了在进入 R ed u c e 节点计算之前，需要把属于一个 R ed u c e 节点的数据归并到一起。 ¨ 解决方案 ¤ 在 Map 阶段进行了 C om b i n i n g 以后，可以根据一定的策略对 Map 输出的中间结 果进行分区 ( p a r t i t i on i n g ) ，这样即可解决以上数据相关性问题避免 R ed u c e 计 算过程中的数据通信。 n 例如：有一个巨大的数组，其最终结果需要排序，每个 Ma p 节点数据处理好后，为了避免在每个 R ed u c e 节点本地排序完成后还需要进行全局排序，我们可以使用一个分区策略如 :( d% R ) ， d 为数据 大小， R 为 R ed u c e 节点的个数，则可根据数据的大小将其划分到指定数据范围的 R ed u c e 节点上，每 个 R ed u c e 将本地数据排好序后即为最终结果。 13 摘要 ¨ G oog l e M a p R ed u c e 的基本工作原理 ¨ 分布式文件系统 GF S 的基本工作原理 ¨ 分布式结构化数据表 Bi g T a b l e 的基本工作原理 分布式文件系统 GF S 的工作原理 ¨ 海量数据怎么存储？数据存储可靠性怎么解决？ ¨ 主流的分布文件系统有： ¤ R ed H a t 的 GFS ¤ I BM 的 G PF S ¤ Su n 的 Lu s t r e 等 ¨ 主要用于对硬件设施要求很高的高性能计算或大型数据中心； ¨ 价格昂贵且缺少完整的数据存储容错解决方案 ¨ 如 Lu s t r e 只对元数据管理提供容错处理，但对于具体的分布存储节点，可靠性完全依赖于这些 分布节点采用 RA I D 或存储区域网 ( SA N ) 技术提供容错，一旦分布节点失效，数据就无法恢复。 15 G oog l e GF S 的基本设计原则 ¨ G oog l e G F S 是一个基于 分布式集群 的大型分布式文件系统，为 M a p R ed u c e 计算框架提供 数据存储和数据可靠性支撑 ； ¨ GF S 是一个构建在分布节点本地文件系统之上的一个逻辑上文件系 统，它将数据存储在物理上分布的每个节点上，但 通过 GF S 将整个 数据形成一个逻辑上整体的文件 。 16 G oog l e GF S 的基本设计原则 17 …… Go o g l e GFS Go o g l e M a p R e d u ce M a p R e d u ce A p p l i ca t i o n s G oog l e GF S 的基本设计原则 ¨ 廉价本地磁盘分布存储 ¤ 各节点本地分布式存储数据，优点是不需要采用价格较贵的集中式磁盘阵列，容量可 随节点数增加自动增加 ¨ 多数据自动备份解决可靠性 ¤ 采用廉价的普通磁盘，把磁盘数据出错视为常态，用 自动多数据备份 存储解决数据存 储可靠性问题 ¨ 为上层的 M a p R ed u c e 计算框架提供支撑 ¤ GFS 作为向上层 M a p R ed u c e 执行框架的底层数据存储支撑，负责处理所有的数据自动存 储和容错处理，因而上层框架不需要考虑底层的数据存储和数据容错问题 18 G oog l e GF S 的基本架构和工作原理 19 Ci t e f r o m Gh e m a w a t et a l . ( S O S P 2 0 0 3 ) G F S M a s t er C h u n k Ser v er GF S M as t e r M a s t er 上保存了 GFS 文件系统的 三种元数据 ： ¨ 命名空间 ( N a m e Sp a c e) 即整个 分布式文件系统的目录结构 ¨ Ch u n k 与文件名的映射表 ¨ Ch u n k 副本的位置信息，每一个 Ch u n k 默认有 3 个副本 20 G F S M a s t er • 前两种元数据可通过操作日志提供容错处理能力； • 第 3 个元数据直接保存在 C h u n k Ser v er 上， M a s t er 启动或 C h u n k Ser v er 注册 时自动完成在 C h u n k Ser v er 上元数据的生成； • 因此，当 M a s t er 失效时，只要 C h u n k Ser v er 数据保存完好，可迅速恢复 M a s t er 上的元数据。 G oog l e GF S 的基本架构和工作原理 ¨ GFS 中每个数据块划分缺省为 64MB ¨ 每个数据块会分别在 3 个 ( 缺省情况下 ) 不同的地 方复制副本； ¨ 对每一个数据块，仅当 3 个副本都更新成功时， 才认为数据保存成功。 21 ¨ 当某个副本失效时， M a s t er 会自动将正确的副本数据进行复制以保证足够的 副本数； ¨ GFS 上存储的数据块副本，在物理上以一个本地的 Li n u x 操作系统的文件形式 存储，每一个数据块再划分为 64KB 的子块，每个子块有一个 32 位的校验和， 读数据时会检查校验和以保证使用未失效的数据。 C h u n k Ser v er GF S ChunkS er v er ： 即用来保存大量实际数据的数据服务器。 G oog l e GF S 的基本架构和工作原理 ¨ 数据访问工作过程 ¤ 1. 在程序运行前，数据已经存储在 GFS 文件系统中；程序运行时应用程序会告诉 G F S Ser v er 所要访问的文件名或者数据块索引是什么 22 G oog l e GF S 的基本架构和工作原理 ¨ 数据访问工作过程 ¤ 2. G F S Ser v er 根据文件名和数据块索引在其文件目录空间中查找和定位该文件或数据块， 找出数据块具体在哪些 C h u n k Ser v er 上；将这些位置信息回送给应用程序 23 G oog l e GF S 的基本架构和工作原理 ¨ 数据访问工作过程 ¤ 3. 应用程序根据 GFS Ser v er 返回的具体 Ch u n k 数据块位置信息，直接访问相应的 C h u n k Ser v er 24 G oog l e GF S 的基本架构和工作原理 ¨ 数据访问工作过程 ¤ 4. 应用程序根据 GFS Ser v er 返回的具体 Ch u n k 数据块位置信息直接读取指定位置的数据 进行计算处理 25 G oog l e GF S 的基本架构和工作原理 ¨ 数据访问工作过程 ¤ 特点：应用程序访问具体数据时不需要经过 G F S M a s t er ，因此，避免了 M a s t er 成为访问瓶颈 ¤ 并发访问：由于一个大数据会存储在不同的 C h u n k Ser v er 中，应用程序可实现并发访问 26 G oog l e GF S 的基本架构和工作原理 ¨ GF S 的系统管理技术 ¤ 大规模集群安装技术： 如何在一个成千上万个节点的集群上迅速部署 GFS ， 升级管理和维护等 ¤ 故障检测技术： GFS 是构建在不可靠的廉价计算机之上的文件系统，节点数 多，故障频繁，如何快速检测、定位、恢复或隔离故障节点 ¤ 节点动态加入技术： 当新的节点加入时，需要能自动安装和部署 GFS ¤ 节能技术： 服务器的耗电成本大于购买成本， G oog l e 为每个节点服务器配 置了蓄电池替代 U PS ，大大节省了能耗。 27 G oog l e GF S 的基本架构和工作原理摘要 ¨ G oog l e M a p R ed u c e 的基本工作原理 ¨ 分布式文件系统 GF S 的基本工作原理 ¨ 分布式结构化数据表 Bi g T a b l e 的基本工作原理 Bi g T a b l e 的基本作用和设计思想 ¨ GFS 是一个文件系统，难以提供对结构化数据的存储和访问管理。为此， G oog l e 在 GFS 之上又设计了一个结构化数据存储和访问管理系统 — Bi g T a b l e ， 为应用程序提供比单纯的文件系统更方便、更高层的数据操作能力。 ¨ G oog l e 的很多数据，包括 We b 索引、卫星图像数据、地图数据等都以结构化形 式存放在 Bi g T a b l e 中。 ¨ Bi g T a b l e 提供了一定粒度的结构化数据操作能力，主要解决一些大型媒体数据 （ We b 文档、图片等）的结构化存储问题。但与传统的关系数据库相比，其结 构化粒度没有那么高，也没有事务处理等能力，因此，它并不是真正意义上的 数据库。 29 Bi g T a b l e 的设计动机和目标  需要存储多种数据 n G oog l e 提供的服务很多，需要处理的数据类型也很多，如 U RL ，网页，图片，地 图数据， em a i l ，用户的个性化设置等  海量的服务请求 n G oog l e 是目前世界上最繁忙的系统，因此，需要有高性能的请求和数据处理能 力  商用数据库无法适用 n 在如此庞大的分布集群上难以有效部署商用数据库系统，且其难以承受如此巨 量的数据存储和操作需求 30 Bi g T a b l e 的设计动机和目标  广泛的适用性： 为一系列服务和应用而设计的数据存储系统，可满足对 不同类型数据的存储和操作需求  很强的可扩展性： 根据需要可随时自动加入或撤销服务器节点  高吞吐量数据访问： 提供 P 级数据存储能力，每秒数百万次的访问请求  高可用性和容错性： 保证系统在各种情况下都能正常运转，服务不中断  自动管理能力： 自动加入和撤销服务器，自动负载平衡  简单性： 系统设计尽量简单以减少复杂性和出错率 31 Bi g T a b l e 数据模型 ¨ Bi g T a b l e 主要是一个分布式多维表，表中的数据通过： ¤ 一个行关键字（ ro w ke y ） ¤ 一个列关键字（ c ol u m n ke y ） ¤ 一个时间戳（ t i m es t a m p ） 进行索引和查询定位的。 p Bi g T a b l e 对存储在表中的数据不做任何解释，一律视为字节串， 具体数据结构的实现由用户自行定义。 p Bi g T a b l e 查询模型 p ( ro w : s t r i n g , c ol u m n : s t r i n g , t i m e: i n t 6 4 ) à 结果数据字节串 p 支持查询、插入和删除操作 32 Bi g T a b l e 数据模型 – We b T a b l e E xa m p l e 33 A l a r g e co l l e ct i o n o f w e b p a g e s a n d r e l a t e d i n f o r m a t i o n A T a b l e i n Bi g T a b l e i s a : S p a r se , D i st r i b u t e d , P e r si st e n t , M u l t i d i m e n si o n a l So r t e d m a p . Bi g T a b l e 数据存储格式 34 Ro w K e y p 行 (R o w ): 大小不超过 64K B 的任意字符串。表中的数据都是 根据行关键字进 行排序 的。 p co m . cn n . w w w 就是一个行关键字，指明一行存储数据。 URL 地址倒排好处是： 1) 同一地 址的网页将被存储在表中连续的位置，便于查找； 2) 倒排便于数据压缩，可大幅提高数 据压缩率 p 子表 (T a b l e t ) ： 一个大表可能太大，不利于存储管理，将在水平方向上被分为 多个子表 Bi g T a b l e 数据存储格式 35 Co l u m n Fa mily C o l u m n f a m i l y i s t h e u n i t o f a cce ss co n t r o l Bi g T a b l e 数据存储格式 36 Co l u m n C o l u m n ke y i s sp e ci f i e d b y “ Co l u mn f a mi l y : qua lif ie r ” Bi g T a b l e 数据存储格式 37 Co l u m n p 列 (C o l u mn ): Bi g T a b l e 将列关键字组织成为“列族” ( co l u m n f a m i l y) ，每个 族中的数据属于同一类别，如 a n ch o r 是一个列族，其下可有不同的表示一 个个超链的列关键字。一个列族下的数据会被压缩在一起存放（按列存 放）。因此，一个列关键字可表示为： 族名：列名 ( f a m i l y: q u a l i f i e r ) p co n t e n t 、 a n ch o r 都是族名；而 cn n si . co m 和 m y . l o o k. ca 则是 a n ch o r 族中 的列名。 Bi g T a b l e 数据存储格式 38 tim es ta m p p 时间戳 ( t i m e st a m p ) : 很多时候同一个 URL 的网页会不断更新，而 Go o g l e 需 要保存不同时间的网页数据，因此需要使用时间戳来加以区分。 p 为了简化不同版本的数据管理， Bi g T a b l e 提供给了两种设置： p 保留最近的 n 个版本数据 p 保留限定时间内的所有不同版本数据 Bi g T a b l e 数据存储格式 39 Ce l l C e l l : t h e st o r a g e r e f e r e n ce d b y a p a r t i cu l a r ro w key , co l u mn key , a n d ti m e s ta m p Bi g T a b l e 基本架构 40 Bi g T a b l e 主服务器 Bi g T a b l e 客户端 Bi g T a b l e 客户端 程序库 Bi g T a b l e 子表服务器 Bi g T a b l e 子表服务器 Bi g T a b l e 子表服务器 Bi g T a b l e 子表服务器 … … 执行元数据操 作和负载平衡 数据存储和 访问操作 数据存储和 访问操作 数据存储和 访问操作 数据存储和 访问操作 GF S Ch u b b y 服务器 （分布式锁服务） G oog l eW or k Q u eu e 负责故障监控和处理 子表数据的存储及日志 元数据存储及主服务器选择 Bi g T a b l e 基本架构 41 主服 务器 新子表 分配 子表服务 器间的负 载均衡 子表服务 器状态 监控 ¤ 主服务器 l 新子表分配：当一个新子表产生时，主服务器通过 加载命令将其分配给一个空间足够大的子表服务器； 创建新表、表合并及较大子表的分裂都会产生新的 子表。 l 子表监控：通过 Ch u b b y 完成。所有子表服务器基本 信息被保存在 Ch u b b y 中的服务器目录中主服务器检 测这个目录可获取最新子表服务器的状态信息。当 子表服务器出现故障，主服务器将终止该子表服务 器，并将其上的全部子表数据移动到其它子表服务 器。 l 负载均衡：当主服务器发现某个子表服务器负载过 重时，将自动对其进行负载均衡操作。 Bi g T a b l e 基本架构 42 ¤ 子表服务器 n Bi g T a b l e 中的数据都以子表形式保存在子 表服务器上，客户端程序也直接和子表服 务器通信。 n 分配：当一个新子表产生，子表服务器的 主要问题包括子表的定位、分配、及子表 数据的最终存储。 ¤ 子表的基本存储结构 SST a b l e n SST a b l e 是 Bi g T a b l e 内部的基本存储结构，以 GFS 文件形式存储在 GFS 文件系 统中。一个 SST a b l e 实际上对应于 GFS 中的一个 64M B 的数据块 (Chunk) n SST a b l e 中的数据进一步划分为 64KB 的子块，因此一个 SST a b l e 可以有多达 1 千个这样的子块。为了维护这些子块的位置信息，需要使用一个 In d e x 索引。 Index 64K b l o ck 64K b l o ck 64K b l o ck SSTabl e Bi g T a b l e 基本架构 43 ¤ 子表数据格式 n 概念上子表是整个大表的多行数据划分后构成。而一个子表服务器上 的子表将进一步由很多个 SST a b l e 构成，每个 SST a b l e 构成最终的在底层 GFS 中的存储单位。 Index 64K b l o ck 64K b l o ck 64K b l o ck SSTabl e Index 64K b l o ck 64K b l o ck 64K b l o ck SSTabl e Ta b le t S t a r t : a a r d va r k End:appl e Bi g T a b l e 基本架构 44 ¤ 子表数据格式 n 一个 SST a b l e 还可以为不同的子表所共享，以避免同样数据的重复存储。 SSTabl e SSTabl e SSTabl e SSTabl e Ta b le t a a r d va r k appl e Ta b le t appl e_tw o_E boat Bi g T a b l e 基本架构 45 ¤ 子表寻址 n 子表地址以 3 级 B+ 树形式进行索引；首先从 Ch u b b y 服务器中取得根子表， 由根子表找到 二级索引子表， 最后获取最终的 SST a b l e 的位置 参考文献 1. J ef f r e y D ea n a n d Sa n j a y G h em a w a t , Ma p R e d u ce : Si m p l i e d Da ta Pr oc e s s i ng on L a r g e C l us t e r s , Pr oc eed i n g s of O SD I 2 0 0 4 . 2. Sa n j a y G h em a w a t , et . al , Th e G o o g le File S y s tem , Pr oc eed i n g s of t h e 19 th A C M Sym p os i u m on O p er a t i n g s ys t em s p r i n c i p l es , SO SP’ 0 3 , O c t . 2003. 3. Fa y Ch a n g , et . al , Bi g t a b l e : a di s t r i b ut e d st o r a g e sy st e m fo r st r u c t u r e d da t a , Pr oc eed i n g s of O SD I 2006. 46 TH A NK YO U","libVersion":"0.2.4","langs":""}